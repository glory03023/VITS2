{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare filelists for LJSpeech dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See: https://github.com/espeak-ng/espeak-ng/blob/master/docs/languages.md\n",
    "dir_data = \"D:\\\\Corpus\\\\Speech\\\\LJSpeech-1.1\"\n",
    "config = \"../config.yaml\"\n",
    "symlink = \"DUMMY1\"\n",
    "n_val = 100\n",
    "n_test = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get hyperparameters from config file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils.hparams import get_hparams_from_file\n",
    "\n",
    "hps = get_hparams_from_file(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read dataset\n",
    "\n",
    "Here `normalized_text` contains numbers in the form of words.\n",
    "\n",
    "**Note**: you may need to replace all `\"|\"` with `\" | \"` in the file `metadata.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>text</th>\n",
       "      <th>normalized_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DUMMY1/LJ001-0001.wav</td>\n",
       "      <td>Printing, in the only sense with which we are ...</td>\n",
       "      <td>Printing, in the only sense with which we are ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DUMMY1/LJ001-0002.wav</td>\n",
       "      <td>in being comparatively modern.</td>\n",
       "      <td>in being comparatively modern.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DUMMY1/LJ001-0003.wav</td>\n",
       "      <td>For although the Chinese took impressions from...</td>\n",
       "      <td>For although the Chinese took impressions from...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DUMMY1/LJ001-0004.wav</td>\n",
       "      <td>produced the block books, which were the immed...</td>\n",
       "      <td>produced the block books, which were the immed...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DUMMY1/LJ001-0005.wav</td>\n",
       "      <td>the invention of movable metal letters in the ...</td>\n",
       "      <td>the invention of movable metal letters in the ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    file                                               text  \\\n",
       "0  DUMMY1/LJ001-0001.wav  Printing, in the only sense with which we are ...   \n",
       "1  DUMMY1/LJ001-0002.wav                     in being comparatively modern.   \n",
       "2  DUMMY1/LJ001-0003.wav  For although the Chinese took impressions from...   \n",
       "3  DUMMY1/LJ001-0004.wav  produced the block books, which were the immed...   \n",
       "4  DUMMY1/LJ001-0005.wav  the invention of movable metal letters in the ...   \n",
       "\n",
       "                                     normalized_text  cleaned_text  \n",
       "0  Printing, in the only sense with which we are ...           NaN  \n",
       "1                     in being comparatively modern.           NaN  \n",
       "2  For although the Chinese took impressions from...           NaN  \n",
       "3  produced the block books, which were the immed...           NaN  \n",
       "4  the invention of movable metal letters in the ...           NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\n",
    "    f\"{dir_data}/metadata.csv\",\n",
    "    sep=r\"|\",\n",
    "    header=None,\n",
    "    names=[\"file\", \"text\", \"normalized_text\", \"cleaned_text\"],\n",
    "    index_col=False,\n",
    "    # converter to add .wav to file name\n",
    "    converters={\"file\": lambda x: f\"{symlink}/{x.strip()}.wav\", \"text\": str.strip, \"normalized_text\": str.strip},\n",
    ")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaners\n",
    "\n",
    "It may take a while, so better to preprocess the text and save it to a file in advance.\n",
    "\n",
    "**Note** `phonemize_text` takes the longest time.`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tokenize_text', 'add_bos_eos']\n",
      "[['phonemize_text'], ['add_spaces']]\n"
     ]
    }
   ],
   "source": [
    "# Get index of tokenize_text\n",
    "text_cleaners = hps.data.text_cleaners\n",
    "\n",
    "token_idx = text_cleaners.index(\"tokenize_text\")\n",
    "token_cleaners = text_cleaners[token_idx:]\n",
    "print(token_cleaners)\n",
    "\n",
    "\n",
    "# Extract phonemize_text\n",
    "def separate_text_cleaners(text_cleaners):\n",
    "    final_list = []\n",
    "    temp_list = []\n",
    "\n",
    "    for cleaner in text_cleaners:\n",
    "        if cleaner == \"phonemize_text\":\n",
    "            if temp_list:\n",
    "                final_list.append(temp_list)\n",
    "            final_list.append([cleaner])\n",
    "            temp_list = []\n",
    "        else:\n",
    "            temp_list.append(cleaner)\n",
    "\n",
    "    if temp_list:\n",
    "        final_list.append(temp_list)\n",
    "\n",
    "    return final_list\n",
    "\n",
    "\n",
    "text_cleaners = text_cleaners[:token_idx]\n",
    "text_cleaners = separate_text_cleaners(text_cleaners)\n",
    "print(text_cleaners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning with ['phonemize_text'] ...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "espeak not installed on your system",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCleaning with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcleaners\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cleaners[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphonemize_text\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 8\u001b[0m     text_norm \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcleaners\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(text_norm):\n",
      "File \u001b[1;32mD:\\Work\\VITS2\\text\\__init__.py:17\u001b[0m, in \u001b[0;36mtokenizer\u001b[1;34m(text, vocab, cleaner_names, language, cleaned_text)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Converts a string of text to a sequence of IDs corresponding to the symbols in the text.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m    text: string to convert to a sequence of IDs\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m    List of integers corresponding to the symbols in the text\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cleaned_text:\n\u001b[1;32m---> 17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_clean_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcleaner_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mint\u001b[39m, text\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)))\n",
      "File \u001b[1;32mD:\\Work\\VITS2\\text\\__init__.py:31\u001b[0m, in \u001b[0;36m_clean_text\u001b[1;34m(text, vocab, cleaner_names, language)\u001b[0m\n\u001b[0;32m     29\u001b[0m     cleaner \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(cleaners, name)\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(cleaner), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown cleaner: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 31\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mcleaner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "File \u001b[1;32mD:\\Work\\VITS2\\text\\cleaners.py:41\u001b[0m, in \u001b[0;36mphonemize_text\u001b[1;34m(text, language, *args, **kwargs)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mphonemize_text\u001b[39m(text: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39margs, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men-us\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mphonemize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlanguage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mespeak\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseparator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseparator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve_empty_lines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve_punctuation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpunctuation_marks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_preserved_symbols_re\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_stress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnjobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kigrs\\.conda\\envs\\speech\\lib\\site-packages\\phonemizer\\phonemize.py:206\u001b[0m, in \u001b[0;36mphonemize\u001b[1;34m(text, language, backend, separator, strip, prepend_text, preserve_empty_lines, preserve_punctuation, punctuation_marks, with_stress, tie, language_switch, words_mismatch, njobs, logger)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;66;03m# initialize the phonemization backend\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mespeak\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 206\u001b[0m     phonemizer \u001b[38;5;241m=\u001b[39m \u001b[43mBACKENDS\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpunctuation_marks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpunctuation_marks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreserve_punctuation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreserve_punctuation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwith_stress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_stress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtie\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtie\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlanguage_switch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlanguage_switch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwords_mismatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwords_mismatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m backend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mespeak-mbrola\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    216\u001b[0m     phonemizer \u001b[38;5;241m=\u001b[39m BACKENDS[backend](\n\u001b[0;32m    217\u001b[0m         language,\n\u001b[0;32m    218\u001b[0m         logger\u001b[38;5;241m=\u001b[39mlogger)\n",
      "File \u001b[1;32mc:\\Users\\kigrs\\.conda\\envs\\speech\\lib\\site-packages\\phonemizer\\backend\\espeak\\espeak.py:45\u001b[0m, in \u001b[0;36mEspeakBackend.__init__\u001b[1;34m(self, language, punctuation_marks, preserve_punctuation, with_stress, tie, language_switch, words_mismatch, logger)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, language: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m     38\u001b[0m              punctuation_marks: Optional[Union[\u001b[38;5;28mstr\u001b[39m, Pattern]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     39\u001b[0m              preserve_punctuation: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m              words_mismatch: WordMismatch \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     44\u001b[0m              logger: Optional[Logger] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 45\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpunctuation_marks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpunctuation_marks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreserve_punctuation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreserve_punctuation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_espeak\u001b[38;5;241m.\u001b[39mset_voice(language)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_with_stress \u001b[38;5;241m=\u001b[39m with_stress\n",
      "File \u001b[1;32mc:\\Users\\kigrs\\.conda\\envs\\speech\\lib\\site-packages\\phonemizer\\backend\\espeak\\base.py:39\u001b[0m, in \u001b[0;36mBaseEspeakBackend.__init__\u001b[1;34m(self, language, punctuation_marks, preserve_punctuation, logger)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, language: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m     36\u001b[0m              punctuation_marks: Optional[Union[\u001b[38;5;28mstr\u001b[39m, Pattern]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     37\u001b[0m              preserve_punctuation: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     38\u001b[0m              logger: Optional[Logger] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpunctuation_marks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpunctuation_marks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreserve_punctuation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreserve_punctuation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_espeak \u001b[38;5;241m=\u001b[39m EspeakWrapper()\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloaded \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_espeak\u001b[38;5;241m.\u001b[39mlibrary_path)\n",
      "File \u001b[1;32mc:\\Users\\kigrs\\.conda\\envs\\speech\\lib\\site-packages\\phonemizer\\backend\\base.py:77\u001b[0m, in \u001b[0;36mBaseBackend.__init__\u001b[1;34m(self, language, punctuation_marks, preserve_punctuation, logger)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# ensure the backend is installed on the system\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m---> 77\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(  \u001b[38;5;66;03m# pragma: nocover\u001b[39;00m\n\u001b[0;32m     78\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m not installed on your system\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname()))\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger \u001b[38;5;241m=\u001b[39m logger\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minitializing backend \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mversion()))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: espeak not installed on your system"
     ]
    }
   ],
   "source": [
    "from text import tokenizer\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "text_norm = data[\"normalized_text\"].tolist()\n",
    "for cleaners in text_cleaners:\n",
    "    print(f\"Cleaning with {cleaners} ...\")\n",
    "    if cleaners[0] == \"phonemize_text\":\n",
    "        text_norm = tokenizer(text_norm, Vocab, cleaners, language=hps.data.language)\n",
    "    else:\n",
    "        for idx, text in enumerate(text_norm):\n",
    "            temp = tokenizer(text, Vocab, cleaners, language=hps.data.language)\n",
    "            text_norm[idx] = temp\n",
    "\n",
    "data = data.assign(cleaned_text=text_norm)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and save vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary: 129\n",
      "['<pad>', '<unk>', '<bos>', '<eos>', '<space>', '<laugh>', 'n', 't', 'ə', 's', 'd', 'ð', 'ɹ', 'k', 'z', 'ɪ', 'l', 'm', 'ˈɪ', 'p', 'w', 'v', 'ˈɛ', 'f', 'ˈeɪ', 'b', 'ɚ', ',', 'ʌ', 'ˈæ', 'h', 'ᵻ', 'i', 'æ', '.', 'ˈaɪ', 'ˈiː', 'ʃ', 'uː', 'ˈoʊ', 'ˈɑː', 'ˈʌ', 'ŋ', 'əl', 'ˈuː', 'ɾ', 'ɡ', 'ɐ', 'ˈɜː', 'dʒ', 'tʃ', 'iː', 'j', 'ˈaʊ', 'θ', 'ˌɪ', 'ˈɔː', 'ˈɔ', 'ˈoːɹ', 'ɔːɹ', 'ɛ', 'ˌɛ', 'ˌʌ', 'ˈɑːɹ', 'ˌæ', 'ˈɔːɹ', 'ˈʊ', 'ɜː', 'oʊ', 'eɪ', 'ˈɛɹ', 'ˈɪɹ', '\"', 'ˌeɪ', 'iə', 'ʊ', 'ˌaɪ', 'ˈɔɪ', 'ˌɑː', ';', 'aɪ', 'ɛɹ', 'ˈʊɹ', 'ɑːɹ', 'ʒ', 'ˈaɪɚ', 'ˌiː', 'ˌuː', 'ˌoʊ', 'aʊ', 'ˈiə', 'ɑː', 'ɔː', 'n̩', 'ʔ', 'ˈaɪə', ':', 'oːɹ', 'ˌaʊ', 'ˌɑːɹ', 'ˌɜː', 'ˌoː', 'ˈoː', '?', 'ˌɔːɹ', 'ˌɔː', 'ɪɹ', 'ʊɹ', 'oː', '!', 'ɔɪ', 'ˌʊɹ', 'ˌʊ', 'ˌiə', 'ˌɔɪ', 'r', 'ɔ', 'ˌoːɹ', 'aɪə', 'ˌɪɹ', 'aɪɚ', 'ˌɔ', 'ˌɛɹ', 'x', '“', '”', 'ˈɚ', 'ˌaɪɚ', 'ˌn̩']\n"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from utils.task import load_vocab, save_vocab\n",
    "from text.symbols import special_symbols, UNK_ID\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def yield_tokens(cleaned_text: List[str]):\n",
    "    for text in cleaned_text:\n",
    "        yield text.split()\n",
    "\n",
    "\n",
    "text_norm = data[\"cleaned_text\"].tolist()\n",
    "vocab = build_vocab_from_iterator(yield_tokens(text_norm), specials=special_symbols)\n",
    "vocab.set_default_index(UNK_ID)\n",
    "\n",
    "vocab_file = f\"../vocab.txt\"\n",
    "save_vocab(vocab, vocab_file)\n",
    "\n",
    "vocab = load_vocab(vocab_file)\n",
    "print(f\"Size of vocabulary: {len(vocab)}\")\n",
    "print(vocab.get_itos())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token cleaners\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>text</th>\n",
       "      <th>normalized_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DUMMY1/LJ001-0001.wav</td>\n",
       "      <td>Printing, in the only sense with which we are ...</td>\n",
       "      <td>Printing, in the only sense with which we are ...</td>\n",
       "      <td>p ɹ ˈɪ n t ɪ ŋ , &lt;space&gt; ˈɪ n &lt;space&gt; ð ə &lt;spa...</td>\n",
       "      <td>2\\t19\\t12\\t18\\t6\\t7\\t15\\t42\\t27\\t4\\t18\\t6\\t4\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DUMMY1/LJ001-0002.wav</td>\n",
       "      <td>in being comparatively modern.</td>\n",
       "      <td>in being comparatively modern.</td>\n",
       "      <td>ˈɪ n &lt;space&gt; b ˈiː ɪ ŋ &lt;space&gt; k ə m p ˈæ ɹ ə ...</td>\n",
       "      <td>2\\t18\\t6\\t4\\t25\\t36\\t15\\t42\\t4\\t13\\t8\\t17\\t19\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DUMMY1/LJ001-0003.wav</td>\n",
       "      <td>For although the Chinese took impressions from...</td>\n",
       "      <td>For although the Chinese took impressions from...</td>\n",
       "      <td>f ɔːɹ &lt;space&gt; ɔː l ð ˈoʊ &lt;space&gt; ð ə &lt;space&gt; t...</td>\n",
       "      <td>2\\t23\\t59\\t4\\t92\\t16\\t11\\t39\\t4\\t11\\t8\\t4\\t50\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DUMMY1/LJ001-0004.wav</td>\n",
       "      <td>produced the block books, which were the immed...</td>\n",
       "      <td>produced the block books, which were the immed...</td>\n",
       "      <td>p ɹ ə d ˈuː s t &lt;space&gt; ð ə &lt;space&gt; b l ˈɑː k ...</td>\n",
       "      <td>2\\t19\\t12\\t8\\t10\\t44\\t9\\t7\\t4\\t11\\t8\\t4\\t25\\t1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DUMMY1/LJ001-0005.wav</td>\n",
       "      <td>the invention of movable metal letters in the ...</td>\n",
       "      <td>the invention of movable metal letters in the ...</td>\n",
       "      <td>ð ə &lt;space&gt; ɪ n v ˈɛ n ʃ ə n &lt;space&gt; ʌ v &lt;spac...</td>\n",
       "      <td>2\\t11\\t8\\t4\\t15\\t6\\t21\\t22\\t6\\t37\\t8\\t6\\t4\\t28...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    file                                               text  \\\n",
       "0  DUMMY1/LJ001-0001.wav  Printing, in the only sense with which we are ...   \n",
       "1  DUMMY1/LJ001-0002.wav                     in being comparatively modern.   \n",
       "2  DUMMY1/LJ001-0003.wav  For although the Chinese took impressions from...   \n",
       "3  DUMMY1/LJ001-0004.wav  produced the block books, which were the immed...   \n",
       "4  DUMMY1/LJ001-0005.wav  the invention of movable metal letters in the ...   \n",
       "\n",
       "                                     normalized_text  \\\n",
       "0  Printing, in the only sense with which we are ...   \n",
       "1                     in being comparatively modern.   \n",
       "2  For although the Chinese took impressions from...   \n",
       "3  produced the block books, which were the immed...   \n",
       "4  the invention of movable metal letters in the ...   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  p ɹ ˈɪ n t ɪ ŋ , <space> ˈɪ n <space> ð ə <spa...   \n",
       "1  ˈɪ n <space> b ˈiː ɪ ŋ <space> k ə m p ˈæ ɹ ə ...   \n",
       "2  f ɔːɹ <space> ɔː l ð ˈoʊ <space> ð ə <space> t...   \n",
       "3  p ɹ ə d ˈuː s t <space> ð ə <space> b l ˈɑː k ...   \n",
       "4  ð ə <space> ɪ n v ˈɛ n ʃ ə n <space> ʌ v <spac...   \n",
       "\n",
       "                                              tokens  \n",
       "0  2\\t19\\t12\\t18\\t6\\t7\\t15\\t42\\t27\\t4\\t18\\t6\\t4\\t...  \n",
       "1  2\\t18\\t6\\t4\\t25\\t36\\t15\\t42\\t4\\t13\\t8\\t17\\t19\\...  \n",
       "2  2\\t23\\t59\\t4\\t92\\t16\\t11\\t39\\t4\\t11\\t8\\t4\\t50\\...  \n",
       "3  2\\t19\\t12\\t8\\t10\\t44\\t9\\t7\\t4\\t11\\t8\\t4\\t25\\t1...  \n",
       "4  2\\t11\\t8\\t4\\t15\\t6\\t21\\t22\\t6\\t37\\t8\\t6\\t4\\t28...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from text import detokenizer\n",
    "\n",
    "text_norm = data[\"cleaned_text\"].tolist()\n",
    "for idx, text in enumerate(text_norm):\n",
    "    temp = tokenizer(text, vocab, token_cleaners, language=hps.data.language)\n",
    "    assert UNK_ID not in temp, f\"Found unknown symbol:\\n{text}\\n{detokenizer(temp)}\"\n",
    "    text_norm[idx] = temp\n",
    "\n",
    "text_norm = [\"\\t\".join(map(str, text)) for text in text_norm]\n",
    "data = data.assign(tokens=text_norm)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save train, val, test filelists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[[\"file\", \"tokens\"]]\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "data_train = data.iloc[n_val + n_test:]\n",
    "data_val = data.iloc[:n_val]\n",
    "data_test = data.iloc[n_val: n_val + n_test]\n",
    "\n",
    "data_train.to_csv(\"../filelists/train.txt\", sep=\"|\", index=False, header=False)\n",
    "data_val.to_csv(\"../filelists/val.txt\", sep=\"|\", index=False, header=False)\n",
    "data_test.to_csv(\"../filelists/test.txt\", sep=\"|\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "naturalspeech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
